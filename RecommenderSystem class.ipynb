{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ed2a3d",
   "metadata": {},
   "source": [
    "Update February 2022 --\n",
    "\n",
    "This is a notebook from my Capstone Rework project; [https://github.com/np1919/Capstone-Rework](https://github.com/np1919/Capstone-Rework). I've left the description at the top to give some context; the class itself could definitely use some work.\n",
    "\n",
    "In the process of eventually deploying an app to `Heroku`, I've come a long way in terms of organizing my ETL structures. I'm grateful for having spent time learning more about coding in Python. This class is far too expensive, computationally, to run on a remote server. \n",
    "\n",
    "That limitation has forced me to consider alternatives -- and structure my 'business outcomes' in a more readable way. I'm going to go back and create several spectra of class labels; and then compare modelling results for each, such that I can find the households most similar to one another in certain areas -- this process in the end may involve more market basket analysis; fpgrowth; and association rules analysis. I'm figuring it out, albeit slowly. \n",
    "\n",
    "In the end I hope to be able to provide several core recommendation tables; for each of 3-5 class labels for households in the data. Moreover; to be able to generate those class labels using new data that might become available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd5c9c",
   "metadata": {},
   "source": [
    "# DunnHumby Recommender System "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9e0e9",
   "metadata": {},
   "source": [
    "Because of the complexity of this dataset, and my weak background in statistics, I opted to showcase a Python class structure capable of generating recommendations for any given household based on their previous purchases. \n",
    "\n",
    "My thinking is that a system like this would be able to effectively identify profitable spending patterns by a household -- those associated products which influence the customer to spend more or purchase more frequently. \n",
    "\n",
    "For example, if our competitor pricing research found that we should target a specific product in order to vie for market share; **the recommender system below would be able to identify what other products might do well alongside the 'targeted' item in a marketing campaign to that customer. With multiple reasons to come to the store, the customer might be more inclined to make the trip, or to spend more if they do**. \n",
    "\n",
    "I recognize that in a brick-and-mortar store, having live-updating recommendations based on items in basket is not necessarily as useful as it would be in an online shop -- however, I believe there is value to be offered by using a system like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e4e70",
   "metadata": {},
   "source": [
    "Data Pipeline\n",
    "---\n",
    "\n",
    "The concept (and practice!) of molding data into a 'pipeline' is a deep one -- the 'correct answer' has to be negotiated based on current data infrastructure, available resources and future scope.\n",
    "\n",
    "An effective data pipeline would:\n",
    "\n",
    "- generate or otherwise acquire data\n",
    "- ensure the data's cleanliness and integrity\n",
    "- seamlessly add the data to an existing data pool\n",
    "- access that data as needed, for analysis and modelling\n",
    "- surface that analysis and modelling results\n",
    "\n",
    "For the scope of this project, we don't have much of a chance to acquire more data. But, if we assume that any other data would be in the same format as the data we've already received (big leap), then we've already made some work on creating a pipeline for future data.\n",
    "\n",
    "**pipeline thus far**:\n",
    "- commodity breakdown/sales analysis\n",
    "- adding datetime\n",
    "- data truncation\n",
    "- customer segmentation (RFM score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a01d7",
   "metadata": {},
   "source": [
    "The pipeline is important, as it allows us to quickly identify changing patterns in customer purchase behaviour in the case of live or newly acquired data in the same format.\n",
    "\n",
    "Adding datetime is a 'housekeeping' issue which makes things easier on our end. \n",
    "\n",
    "Customer segmentation, however, provides some valuable insight about any given customer (household) in our data. \n",
    "\n",
    "By segmenting the customers in various ways, we can create effective recommendations for a given subset of our customers -- for example, the most profitable ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86e8c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (16,6)\n",
    "import datetime\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "import my_funcs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662cbf35",
   "metadata": {},
   "source": [
    "Forming `merged` from `product.csv` and `transaction_data.csv`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "399d05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transactions\n",
    "# import product info\n",
    "# join commodity_desc\n",
    "\n",
    "transactions = pd.read_csv('data/transaction_data.csv')\n",
    "product = pd.read_csv('data/product.csv')\n",
    "merged = transactions.merge(product[['PRODUCT_ID', 'COMMODITY_DESC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bb1b19e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirming christmas 1 is at DAY 278: 2004-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# calling my module functions to map Section Labels and Datetime\n",
    "\n",
    "merged['Section Labels'] = my_funcs.return_section_labels(merged)\n",
    "\n",
    "my_funcs.add_datetime(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd2e8c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirming christmas 1 is at DAY 278: 2004-12-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "my_funcs.add_datetime(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb79ece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2004-03-23 16:31:00\n",
       "1         2004-03-26 17:55:00\n",
       "2         2004-03-26 00:35:00\n",
       "3         2004-03-27 15:51:00\n",
       "4         2004-03-29 23:03:00\n",
       "                  ...        \n",
       "2595727   2006-03-03 15:20:00\n",
       "2595728   2006-03-03 15:20:00\n",
       "2595729   2006-03-03 15:20:00\n",
       "2595730   2006-03-03 15:20:00\n",
       "2595731   2006-03-03 15:20:00\n",
       "Name: datetime, Length: 2595732, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "55a6a7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm each basket has a unique datetime index..\n",
    "\n",
    "all(merged.groupby('BASKET_ID')['datetime'].nunique()) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e468b9",
   "metadata": {},
   "source": [
    "# Recommender System for Households\n",
    "---\n",
    "\n",
    "Using FPGrowth (apriori and association rules)\n",
    "\n",
    "Below I've implemented a rudimentary (minimum viable product?) Recommender System class.\n",
    "\n",
    "There is so much left for me to learn about how a class like this might be integrated into 'production'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0b0bd77",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RecommenderSystem:\n",
    "    '''\n",
    "    ## hh_key :  the household_key\n",
    "    ## df : the transactions df; \n",
    "    ## column : the column in df to be used for MBA ('COMMODITY_DESC, DEPARTMENT, SUB_COMMODITY_DESC')\n",
    "    ## max_len :  max length of any antecedent/consequent chains in support_table\n",
    "    ## support_threshold : minimum 'support' threshold to generate fpgrowth\n",
    "    \n",
    "    ## metric : the association rules metric to maximize\n",
    "    ## assoc_threshold : the association rules threshold, given the metric.\n",
    "    '''\n",
    "        ## Instantiate Class\n",
    "\n",
    "    def __init__(self, \n",
    "                 hh_key,   ### FOR HOUSEHOLDS#!### \n",
    "                 df=merged, \n",
    "                 column='COMMODITY_DESC', \n",
    "                    max_len=None, ### CONSIDER REDUCING THIS VALUE FOR SIMPLICITY ###\n",
    "                 support_threshold=0.05, ### WITH DATA OF FIXED SIZE, NOT A CONCERN? ###\n",
    "                metric='confidence', \n",
    "                 assoc_threshold=0.8,\n",
    "                ):\n",
    "                                    #TODO: implement intelligent thresholds\n",
    "        self.hh = hh_key\n",
    "        self.metric = metric\n",
    "        self.assoc_threshold=assoc_threshold\n",
    "        self.column = column\n",
    "        self.support_threshold = support_threshold\n",
    "        self.df = df[df['household_key'] == self.hh] # self.df is transactions for this customer only\n",
    "        self.max_len = max_len\n",
    "        \n",
    "\n",
    "        # create support table upon instantiation\n",
    "        \n",
    "        self.get_support_table()\n",
    "     \n",
    "    \n",
    "    ### Support Table Function ###\n",
    "    # uses fpgrowth to generate a support table\n",
    "    def get_support_table(self):\n",
    "        '''Return the support table for `BASKET_ID`s using `column` as product lists\n",
    "        Note: 'BASKET_ID' is hardcoded...\n",
    "        \n",
    "        '''\n",
    "        # create product lists for each basket                                   \n",
    "        product_lists = self.df.groupby('BASKET_ID')[self.column].apply(list) # apply list constructor\n",
    " \n",
    "        # dummy encoding...\n",
    "        te = TransactionEncoder()\n",
    "        te_fit = te.fit_transform(product_lists.values, sparse=True) # encode each \n",
    "        te_df = pd.DataFrame.sparse.from_spmatrix(te_fit, columns=[str(i) for i in te.columns_])\n",
    "      \n",
    "        # fpgrowth table\n",
    "        frequent_itemsets = fpgrowth(te_df, \n",
    "                                    min_support=self.support_threshold, #can alter self.support_threshold\n",
    "                                    use_colnames=True, \n",
    "#                                     verbose=True, \n",
    "                                    max_len=self.max_len,   # can alter self.max_len here.\n",
    "                                    #, low_memory=True,                                   \n",
    "                                    )\n",
    "        # adding a length column for posterity and filtering\n",
    "        frequent_itemsets['size'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "        \n",
    "        # save variable for reference...\n",
    "        self.support_table = frequent_itemsets\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def assoc_table(self):\n",
    "        '''change self.metric, self.assoc_threshold to rank differently'''\n",
    "        ##  calling association rules on our support table\n",
    "        rules = association_rules(self.support_table, metric=self.metric, min_threshold=self.assoc_threshold)\n",
    "        rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "        return rules\n",
    "    \n",
    "    def recommend(self, prev_purchases:list, howmany=5):\n",
    "        '''meat and bones of the recommender system...\n",
    "        accepts:\n",
    "            prev_purchases: a list of previously purchased items\n",
    "            howmany: (int) how many recommendations you want\n",
    "            \n",
    "        returns:\n",
    "            a series consisting of the top 5 results given the self.metric value.\n",
    "            '''\n",
    "        search_terms = list(prev_purchases) # handles frozensets?\n",
    "        # apply list to 'antecedent' in self.assoc_table\n",
    "        search_series = pd.Series(self.assoc_table['antecedents'].apply(list)) \n",
    "        \n",
    "        print(f'Searching for {search_terms}...')\n",
    "        indexes_of_matches = []\n",
    "\n",
    "        # for each antecedent chain...\n",
    "        for item in search_terms:\n",
    "            # iterate through the list of \"antecedent\" rows **search_series**\n",
    "            for idx, val in search_series.iteritems():\n",
    "                if item in val: # if the item is in the row (list of antecedents)..\n",
    "                    indexes_of_matches.append(idx)\n",
    "\n",
    "        rules = self.assoc_table.loc[indexes_of_matches]\n",
    "\n",
    "\n",
    "        # RETURN TOP 5 LIFT CONSEQUENTS\n",
    "        return rules.sort_values(self.metric, ascending=False)[:howmany]['consequents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be20c59",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "---\n",
    "\n",
    "For `household_key == 2`, looking at 'confidence':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c457bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\n",
      "Searching for ['APPLES']...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1429                  (BAKED BREAD/BUNS/ROLLS)\n",
       "185415                (BAKED BREAD/BUNS/ROLLS)\n",
       "185403                 (YOGURT, COOKIES/CONES)\n",
       "185405    (BAKED BREAD/BUNS/ROLLS, MARGARINES)\n",
       "185406        (YOGURT, BAKED BREAD/BUNS/ROLLS)\n",
       "Name: consequents, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh2 = RecommenderSystem(hh_key=2, metric='confidence', assoc_threshold=0.8)#, max_len=2)\n",
    "\n",
    "print(hh2.metric)\n",
    "hh2.recommend(['APPLES'])\n",
    "# returns the top 5 'lift' of all purchases with 'APPLES' antecedent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af54c4",
   "metadata": {},
   "source": [
    "Looking at 'lift'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd7895a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lift\n",
      "Searching for ['APPLES']...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "363522    (FRUIT - SHELF STABLE, CRACKERS/MISC BKD FD, F...\n",
       "385436                  (YOGURT, COOKIES/CONES, MARGARINES)\n",
       "386339    (CANNED JUICES, BAKED BREAD/BUNS/ROLLS, COOKIE...\n",
       "385501         (YOGURT, BAKED BREAD/BUNS/ROLLS, MARGARINES)\n",
       "385498                  (YOGURT, COOKIES/CONES, MARGARINES)\n",
       "Name: consequents, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh2 = RecommenderSystem(hh_key=2, metric='lift', assoc_threshold=0.8)#, max_len=2)\n",
    "print(hh2.metric)\n",
    "hh2.recommend(['APPLES'])\n",
    "# returns the top 5 'lift' of all purchases with 'APPLES' antecedent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371dcf2a",
   "metadata": {},
   "source": [
    "Looking at 'conviction':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7099e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for ['APPLES']...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "222012                         (SOFT DRINKS, COOKIES/CONES)\n",
       "363637    (FRUIT - SHELF STABLE, BAKED BREAD/BUNS/ROLLS,...\n",
       "363576    (FRUIT - SHELF STABLE, SOFT DRINKS, FLUID MILK...\n",
       "363583    (CRACKERS/MISC BKD FD, SOFT DRINKS, FLUID MILK...\n",
       "386400            (YOGURT, MARGARINES, FLUID MILK PRODUCTS)\n",
       "Name: consequents, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh2.metric='conviction'\n",
    "hh2.recommend(['APPLES']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97af212",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "---\n",
    "\n",
    "For multi-item antecedent chains (items in 'basket'), this recommender searches for recommendations for each item independently, then concatenates the full table of results and sorts by the metric currently selected in the class .metric attribute.\n",
    "\n",
    "So many things to consider, with a technology like this. Does it make sense to filter more stringently, and only look for chains of antecedents precisely equal to the search term? What about finding similar products using something like word2vec or other NLP? How do we change or vary our recommendations, so that customers aren't constantly seeing the same recommendations?\n",
    "\n",
    "We could use cluster labels, RFM score, demographics to populate a 'similar customer' association rules table, and make recommendations from there. A basic approach would be to recommend based on recently purchased items; or to reverse-engineer this process and recommend 'trigger' products which we might want to be selling to them.\n",
    "\n",
    "We're getting redundant values in our consequent chains when parsing 5 straight from the top of the `confidence` metric. What about returning the results from a blend of metrics; another option would be to add another layer of logic to prevent redundant recommendations. (create a 'set', and don't stop filling it until it reaches 5 items). \n",
    "\n",
    "One issue about having an antecedent 'chain' instead of a single result: the recommendations won't always be obviously related. This could be frustrating for a customer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72cbad",
   "metadata": {},
   "source": [
    "Run the recommender on all of the purchases by a household using the support table of similarly-labeled customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649d636",
   "metadata": {},
   "source": [
    "Alternative Methods of Recommendation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f262174",
   "metadata": {},
   "source": [
    "- Instead of using the fpgrowth table for each household proper; use the Cluster label (or RFM score group) to generate a 'similar-customers' fpgrowth table; call .recommend() using that table, for the most recent purchases by the household.\n",
    "\n",
    "- Search a customer's history to see their 'DEPARTMENT' or 'COMMODITY' spending vs the average? \n",
    "\n",
    "- Get the rows for each top ranked value in a given support table.. #TODO: deal with ties, inf \n",
    "\n",
    "- Use NLP or regex to find similar items and make the search terms more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4678b",
   "metadata": {},
   "source": [
    "## Second Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4bca1a",
   "metadata": {},
   "source": [
    "Expanding on the functionality\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e79e6",
   "metadata": {},
   "source": [
    "By altering the last few lines of the recommend() function, we could instead return a `set` of items from the top of our results; or add several recommender metrics together and return the set of all items. for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f30f4a08",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class RecommenderSystemSET:\n",
    "    '''\n",
    "    ## hh_key :  the household_key\n",
    "    ## df : the transactions df; \n",
    "    ## column : the column in df to be used for MBA ('COMMODITY_DESC, DEPARTMENT, SUB_COMMODITY_DESC')\n",
    "    ## max_len :  max length of any antecedent/consequent chains in support_table\n",
    "    ## support_threshold : minimum 'support' threshold to generate fpgrowth\n",
    "    \n",
    "    ## metric : the association rules metric to maximize\n",
    "    ## assoc_threshold : the association rules threshold, given the metric.\n",
    "    '''\n",
    "        ## Instantiate Class\n",
    "\n",
    "    def __init__(self, \n",
    "                 hh_key,   ### FOR HOUSEHOLDS#!### \n",
    "                 df=merged, \n",
    "                 column='COMMODITY_DESC', \n",
    "                    max_len=None, ### CONSIDER REDUCING THIS VALUE FOR SIMPLICITY ###\n",
    "                 support_threshold=0.05, ### WITH DATA OF FIXED SIZE, NOT A CONCERN? ###\n",
    "                metric='confidence', \n",
    "                 assoc_threshold=0.8,\n",
    "                ):\n",
    "                                    #TODO: implement intelligent thresholds\n",
    "        self.hh = hh_key\n",
    "        self.metric = metric\n",
    "        self.assoc_threshold=assoc_threshold\n",
    "        self.column = column\n",
    "        self.support_threshold = support_threshold\n",
    "        self.df = df[df['household_key'] == self.hh] # self.df is transactions for this customer only\n",
    "        self.max_len = max_len\n",
    "        \n",
    "\n",
    "        # create support table upon instantiation\n",
    "        \n",
    "        self.get_support_table()\n",
    "     \n",
    "    \n",
    "    ### Support Table Function ###\n",
    "    \n",
    "    # uses fpgrowth to generate a support table\n",
    "    \n",
    "    def get_support_table(self):\n",
    "        '''Return the support table for `BASKET_ID`s using `column` as product lists\n",
    "        Note: 'BASKET_ID' is hardcoded...\n",
    "        \n",
    "        '''\n",
    "        # create product lists for each basket                                   \n",
    "        product_lists = self.df.groupby('BASKET_ID')[self.column].apply(list) # apply list constructor\n",
    " \n",
    "        # dummy encoding...\n",
    "        te = TransactionEncoder()\n",
    "        te_fit = te.fit_transform(product_lists.values, sparse=True) # encode each \n",
    "        te_df = pd.DataFrame.sparse.from_spmatrix(te_fit, columns=[str(i) for i in te.columns_])\n",
    "      \n",
    "        # fpgrowth table\n",
    "        frequent_itemsets = fpgrowth(te_df, \n",
    "                                    min_support=self.support_threshold, #can alter self.support_threshold\n",
    "                                    use_colnames=True, \n",
    "#                                     verbose=True, \n",
    "                                    max_len=self.max_len,   # can alter self.max_len here.\n",
    "                                    #, low_memory=True,                                   \n",
    "                                    )\n",
    "        # adding a length column for posterity and filtering\n",
    "        frequent_itemsets['size'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "        \n",
    "        # save variable for reference...\n",
    "        self.support_table = frequent_itemsets\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def assoc_table(self):\n",
    "        '''change self.metric, self.assoc_threshold to rank differently'''\n",
    "        ##  calling association rules on our support table\n",
    "        rules = association_rules(self.support_table, metric=self.metric, min_threshold=self.assoc_threshold)\n",
    "        rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "        return rules\n",
    "    \n",
    "    \n",
    "    def recommend(self, prev_purchases:list, howmany=5):\n",
    "        '''Edited Function\n",
    "            '''\n",
    "\n",
    "        search_terms = list(prev_purchases) # handles frozensets?\n",
    "        # apply list to 'antecedent' in self.assoc_table\n",
    "        search_series = pd.Series(self.assoc_table['antecedents'].apply(list)) \n",
    "\n",
    "        print(f'Searching for {search_terms}...')\n",
    "        indexes_of_matches = []\n",
    "\n",
    "        # for each antecedent chain...\n",
    "        for item in search_terms:\n",
    "            # iterate through the list of \"antecedent\" rows **search_series**\n",
    "            for idx, val in search_series.iteritems():\n",
    "                if item in val: # if the item is in the row (list of antecedents)..\n",
    "                    indexes_of_matches.append(idx)\n",
    "\n",
    "        rules = self.assoc_table.loc[indexes_of_matches]\n",
    "\n",
    "        # RETURN TOP 5 LIFT CONSEQUENTS\n",
    "        results = []\n",
    "        \n",
    "        \n",
    "        # TODO: check if the tables are populating correctly given only self.metric...\n",
    "        for metric in ['confidence', 'conviction', 'lift', 'leverage', 'support']:\n",
    "            results.extend(list(rules.sort_values(metric, ascending=False)[:howmany]['consequents']))\n",
    "\n",
    "        set_results = []\n",
    "        for x in results:\n",
    "            set_results.extend(list(x))\n",
    "        #       \n",
    "        return set(set_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b00b8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh2 = RecommenderSystemSET(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "63925307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADULT INCONTINENCE',\n",
       " 'AIR CARE',\n",
       " 'ANALGESICS',\n",
       " 'APPLES',\n",
       " 'BABY FOODS',\n",
       " 'BACON',\n",
       " 'BAG SNACKS',\n",
       " 'BAKED BREAD/BUNS/ROLLS',\n",
       " 'BAKED SWEET GOODS',\n",
       " 'BAKING',\n",
       " 'BAKING MIXES',\n",
       " 'BAKING NEEDS',\n",
       " 'BATH',\n",
       " 'BATH TISSUES',\n",
       " 'BATTERIES',\n",
       " 'BEEF',\n",
       " 'BEERS/ALES',\n",
       " 'BERRIES',\n",
       " 'BLEACH',\n",
       " 'BREAD',\n",
       " 'BREAKFAST SAUSAGE/SANDWICHES',\n",
       " 'BREAKFAST SWEETS',\n",
       " 'BROCCOLI/CAULIFLOWER',\n",
       " 'BROOMS AND MOPS',\n",
       " 'CANDY - CHECKLANE',\n",
       " 'CANDY - PACKAGED',\n",
       " 'CANNED JUICES',\n",
       " 'CARROTS',\n",
       " 'CHEESE',\n",
       " 'CHEESES',\n",
       " 'CHICKEN',\n",
       " 'CHRISTMAS  SEASONAL',\n",
       " 'CIGARETTES',\n",
       " 'CITRUS',\n",
       " 'COFFEE',\n",
       " 'COFFEE FILTERS',\n",
       " 'COLD CEREAL',\n",
       " 'CONDIMENTS/SAUCES',\n",
       " 'CONVENIENT BRKFST/WHLSM SNACKS',\n",
       " 'COOKIES/CONES',\n",
       " 'COOKWARE & BAKEWARE',\n",
       " 'CORN',\n",
       " 'COUPON/MISC ITEMS',\n",
       " 'CRACKERS/MISC BKD FD',\n",
       " 'DELI MEATS',\n",
       " 'DEODORANTS',\n",
       " 'DINNER MXS:DRY',\n",
       " 'DISHWASH DETERGENTS',\n",
       " 'DISPOSIBLE FOILWARE',\n",
       " 'DOG FOODS',\n",
       " 'DRIED FRUIT',\n",
       " 'DRY BN/VEG/POTATO/RICE',\n",
       " 'DRY MIX DESSERTS',\n",
       " 'DRY NOODLES/PASTA',\n",
       " 'EGGS',\n",
       " 'ELECTRICAL SUPPPLIES',\n",
       " 'FACIAL TISS/DNR NAPKIN',\n",
       " 'FAMILY PLANNING',\n",
       " 'FD WRAPS/BAGS/TRSH BG',\n",
       " 'FEMININE HYGIENE',\n",
       " 'FIRST AID PRODUCTS',\n",
       " 'FLORAL-FLOWERING PLANTS',\n",
       " 'FLOUR & MEALS',\n",
       " 'FLUID MILK PRODUCTS',\n",
       " 'FROZEN BREAD/DOUGH',\n",
       " 'FROZEN PIE/DESSERTS',\n",
       " 'FROZEN PIZZA',\n",
       " 'FRUIT - SHELF STABLE',\n",
       " 'FRZN BREAKFAST FOODS',\n",
       " 'FRZN FRUITS',\n",
       " 'FRZN MEAT/MEAT DINNERS',\n",
       " 'FRZN NOVELTIES/WTR ICE',\n",
       " 'FRZN VEGETABLE/VEG DSH',\n",
       " 'GRAPES',\n",
       " 'GREETING CARDS/WRAP/PARTY SPLY',\n",
       " 'HAIR CARE ACCESSORIES',\n",
       " 'HAIR CARE PRODUCTS',\n",
       " 'HAND/BODY/FACIAL PRODUCTS',\n",
       " 'HARDWARE SUPPLIES',\n",
       " 'HEAT/SERVE',\n",
       " 'HERBS',\n",
       " 'HISPANIC',\n",
       " 'HOSIERY/SOCKS',\n",
       " 'HOT CEREAL',\n",
       " 'HOT DOGS',\n",
       " 'HOUSEHOLD CLEANG NEEDS',\n",
       " 'ICE CREAM/MILK/SHERBTS',\n",
       " 'LAUNDRY ADDITIVES',\n",
       " 'LAUNDRY DETERGENTS',\n",
       " 'LUNCHMEAT',\n",
       " 'MAKEUP AND TREATMENT',\n",
       " 'MARGARINES',\n",
       " 'MEAT - MISC',\n",
       " 'MILK BY-PRODUCTS',\n",
       " 'MISC. DAIRY',\n",
       " 'MOLASSES/SYRUP/PANCAKE MIXS',\n",
       " 'MUSHROOMS',\n",
       " 'OLIVES',\n",
       " 'ONIONS',\n",
       " 'ORAL HYGIENE PRODUCTS',\n",
       " 'ORGANICS FRUIT & VEGETABLES',\n",
       " 'PAPER HOUSEWARES',\n",
       " 'PAPER TOWELS',\n",
       " 'PASTA SAUCE',\n",
       " 'PET CARE SUPPLIES',\n",
       " 'PLASTIC HOUSEWARES',\n",
       " 'PNT BTR/JELLY/JAMS',\n",
       " 'PORK',\n",
       " 'POTATOES',\n",
       " 'PROCESSED',\n",
       " 'PWDR/CRYSTL DRNK MX',\n",
       " 'REFRGRATD DOUGH PRODUCTS',\n",
       " 'REFRGRATD JUICES/DRNKS',\n",
       " 'RICE CAKES',\n",
       " 'ROLLS',\n",
       " 'SALAD MIX',\n",
       " 'SALD DRSNG/SNDWCH SPRD',\n",
       " 'SEAFOOD - FROZEN',\n",
       " 'SEAFOOD - SHELF STABLE',\n",
       " 'SHAVING CARE PRODUCTS',\n",
       " 'SHORTENING/OIL',\n",
       " 'SOAP - LIQUID & BAR',\n",
       " 'SOFT DRINKS',\n",
       " 'SOUP',\n",
       " 'SPICES & EXTRACTS',\n",
       " 'SPRING/SUMMER SEASONAL',\n",
       " 'SQUASH',\n",
       " 'STATIONERY & SCHOOL SUPPLIES',\n",
       " 'STONE FRUIT',\n",
       " 'SUGARS/SWEETNERS',\n",
       " 'TEAS',\n",
       " 'TOBACCO OTHER',\n",
       " 'TOMATOES',\n",
       " 'TROPICAL FRUIT',\n",
       " 'VALUE ADDED FRUIT',\n",
       " 'VALUE ADDED VEGETABLES',\n",
       " 'VEGETABLES - ALL OTHERS',\n",
       " 'VEGETABLES - SHELF STABLE',\n",
       " 'WAREHOUSE SNACKS',\n",
       " 'WATER - CARBONATED/FLVRD DRINK',\n",
       " 'YOGURT'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(merged[merged['household_key']==2]['COMMODITY_DESC']) # all previous purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10a5b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "525c5643",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PET CARE SUPPLIES',\n",
       " 'HAND/BODY/FACIAL PRODUCTS',\n",
       " 'DOG FOODS',\n",
       " 'CANDY - PACKAGED',\n",
       " 'PET CARE SUPPLIES']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent = list(merged[merged['household_key']==2]['COMMODITY_DESC'])[-5:]\n",
    "most_recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83f2a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh2.metric = 'lift'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "87d3ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for ['PET CARE SUPPLIES', 'HAND/BODY/FACIAL PRODUCTS', 'DOG FOODS', 'CANDY - PACKAGED', 'PET CARE SUPPLIES']...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BAKED BREAD/BUNS/ROLLS',\n",
       " 'BAKED SWEET GOODS',\n",
       " 'BAKING MIXES',\n",
       " 'BAKING NEEDS',\n",
       " 'BEEF',\n",
       " 'COOKIES/CONES',\n",
       " 'DEODORANTS',\n",
       " 'DOG FOODS',\n",
       " 'FLUID MILK PRODUCTS',\n",
       " 'PET CARE SUPPLIES',\n",
       " 'POTATOES',\n",
       " 'SOFT DRINKS',\n",
       " 'VEGETABLES - SHELF STABLE'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh2.recommend(most_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ce7cfb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for ['BAKED BREAD/BUNS/ROLLS']...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'APPLES',\n",
       " 'BAKED SWEET GOODS',\n",
       " 'BAKING MIXES',\n",
       " 'BAKING NEEDS',\n",
       " 'BEEF',\n",
       " 'CANNED JUICES',\n",
       " 'COOKIES/CONES',\n",
       " 'FLUID MILK PRODUCTS',\n",
       " 'MARGARINES',\n",
       " 'ONIONS',\n",
       " 'PAPER HOUSEWARES',\n",
       " 'POTATOES',\n",
       " 'SOFT DRINKS',\n",
       " 'SUGARS/SWEETNERS',\n",
       " 'VEGETABLES - SHELF STABLE',\n",
       " 'YOGURT'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh2.recommend(['BAKED BREAD/BUNS/ROLLS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ba99a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>antecedent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(BAG SNACKS)</td>\n",
       "      <td>(SALADS/DIPS)</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.809524</td>\n",
       "      <td>0.07375</td>\n",
       "      <td>2.475000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(SALADS/DIPS)</td>\n",
       "      <td>(BAG SNACKS)</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>3.809524</td>\n",
       "      <td>0.07375</td>\n",
       "      <td>1.983333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     antecedents    consequents  antecedent support  consequent support  \\\n",
       "0   (BAG SNACKS)  (SALADS/DIPS)               0.150               0.175   \n",
       "1  (SALADS/DIPS)   (BAG SNACKS)               0.175               0.150   \n",
       "\n",
       "   support  confidence      lift  leverage  conviction  antecedent_len  \n",
       "0      0.1    0.666667  3.809524   0.07375    2.475000               1  \n",
       "1      0.1    0.571429  3.809524   0.07375    1.983333               1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh5 = RecommenderSystem(hh_key=5, support_threshold=0.1) # consider opening this range further\n",
    "hh5.metric='confidence'\n",
    "hh5.assoc_threshold=0.2 # very low. \n",
    "\n",
    "hh5.assoc_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccad928",
   "metadata": {},
   "source": [
    "# Association Rules Metrics \n",
    "\n",
    "There is a ton of information that can be gathered by this sort of analysis;\n",
    "\n",
    "By calculating the 'frequency of occurrence' (`support`) for each item (in this case, `COMMODITY_DESC`) as such, we can then perform calculations to determine **if the presence of one product in the purchase basket** has affected the chance that **another product will be purchased** ; by how much (`lift` factor); and how strong the association is (`confidence` proportion).\n",
    "\n",
    "Below we can see some of the calculations quickly using the `association_rules` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718cf87",
   "metadata": {},
   "source": [
    "\n",
    "For example, one top-rated association in the `lift` category from the table above is [butter and pasta sauce] -> [dry noodles and cheese]. This means that the chance of having dry noodles AND cheese in your basket -- if you have butter AND pasta sauce *already* -- is increased by a factor of almost 16 times compared to the original `supports` of those chains of items independent of one another. \n",
    "\n",
    "If instead we sort the table by `confidence`, we can look at the proportion of co-occurence the `antecedent` (chain) exists on it's own.\n",
    "\n",
    "We create this table below using a threshold for the `confidence` metric; calculated as: how many baskets does the `consequent` appear in **with** the `antecedent`, divided by the total number of baskets in which the `antecedent` is present.\n",
    "\n",
    "A confidence score approaching 1 reflects that the `consequent` is found in the `antecedent`'s baskets\n",
    "approaching --> 100% of the time. \n",
    "\n",
    "Below we can see the highest-`confidence` relationships from the the purchases in our data, and the categories in `COMMODITY_DESC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b452b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>antecedent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169500</th>\n",
       "      <td>(CANNED JUICES, BAKED BREAD/BUNS/ROLLS, BAKING...</td>\n",
       "      <td>(POTATOES, FLUID MILK PRODUCTS, VEGETABLES - S...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225623</th>\n",
       "      <td>(YOGURT, BAKING NEEDS)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225621</th>\n",
       "      <td>(YOGURT, BAKING NEEDS, BEEF)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225620</th>\n",
       "      <td>(YOGURT, SOFT DRINKS, BEEF)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225619</th>\n",
       "      <td>(YOGURT, SOFT DRINKS, BAKING NEEDS)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225618</th>\n",
       "      <td>(YOGURT, COOKIES/CONES, BEEF)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225617</th>\n",
       "      <td>(YOGURT, BAKING NEEDS, COOKIES/CONES)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.059259</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225616</th>\n",
       "      <td>(YOGURT, BAKED BREAD/BUNS/ROLLS, BEEF)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, COOKIES/CO...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225615</th>\n",
       "      <td>(YOGURT, BAKED BREAD/BUNS/ROLLS, BAKING NEEDS)</td>\n",
       "      <td>(BAKING MIXES, FLUID MILK PRODUCTS, COOKIES/CO...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225614</th>\n",
       "      <td>(YOGURT, BEEF, FLUID MILK PRODUCTS)</td>\n",
       "      <td>(BAKING MIXES, BAKED BREAD/BUNS/ROLLS, COOKIES...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225613</th>\n",
       "      <td>(YOGURT, BAKING NEEDS, FLUID MILK PRODUCTS)</td>\n",
       "      <td>(BAKING MIXES, BAKED BREAD/BUNS/ROLLS, COOKIES...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225612</th>\n",
       "      <td>(YOGURT, BAKING MIXES, BEEF)</td>\n",
       "      <td>(FLUID MILK PRODUCTS, BAKED BREAD/BUNS/ROLLS, ...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>inf</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              antecedents  \\\n",
       "169500  (CANNED JUICES, BAKED BREAD/BUNS/ROLLS, BAKING...   \n",
       "225623                             (YOGURT, BAKING NEEDS)   \n",
       "225621                       (YOGURT, BAKING NEEDS, BEEF)   \n",
       "225620                        (YOGURT, SOFT DRINKS, BEEF)   \n",
       "225619                (YOGURT, SOFT DRINKS, BAKING NEEDS)   \n",
       "225618                      (YOGURT, COOKIES/CONES, BEEF)   \n",
       "225617              (YOGURT, BAKING NEEDS, COOKIES/CONES)   \n",
       "225616             (YOGURT, BAKED BREAD/BUNS/ROLLS, BEEF)   \n",
       "225615     (YOGURT, BAKED BREAD/BUNS/ROLLS, BAKING NEEDS)   \n",
       "225614                (YOGURT, BEEF, FLUID MILK PRODUCTS)   \n",
       "225613        (YOGURT, BAKING NEEDS, FLUID MILK PRODUCTS)   \n",
       "225612                       (YOGURT, BAKING MIXES, BEEF)   \n",
       "\n",
       "                                              consequents  antecedent support  \\\n",
       "169500  (POTATOES, FLUID MILK PRODUCTS, VEGETABLES - S...            0.066667   \n",
       "225623  (BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...            0.066667   \n",
       "225621  (BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...            0.066667   \n",
       "225620  (BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...            0.066667   \n",
       "225619  (BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...            0.066667   \n",
       "225618  (BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...            0.066667   \n",
       "225617  (BAKING MIXES, FLUID MILK PRODUCTS, BAKED BREA...            0.066667   \n",
       "225616  (BAKING MIXES, FLUID MILK PRODUCTS, COOKIES/CO...            0.066667   \n",
       "225615  (BAKING MIXES, FLUID MILK PRODUCTS, COOKIES/CO...            0.066667   \n",
       "225614  (BAKING MIXES, BAKED BREAD/BUNS/ROLLS, COOKIES...            0.066667   \n",
       "225613  (BAKING MIXES, BAKED BREAD/BUNS/ROLLS, COOKIES...            0.066667   \n",
       "225612  (FLUID MILK PRODUCTS, BAKED BREAD/BUNS/ROLLS, ...            0.066667   \n",
       "\n",
       "        consequent support   support  confidence   lift  leverage  conviction  \\\n",
       "169500            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225623            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225621            0.111111  0.066667         1.0   9.00  0.059259         inf   \n",
       "225620            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225619            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225618            0.111111  0.066667         1.0   9.00  0.059259         inf   \n",
       "225617            0.111111  0.066667         1.0   9.00  0.059259         inf   \n",
       "225616            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225615            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225614            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225613            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "225612            0.088889  0.066667         1.0  11.25  0.060741         inf   \n",
       "\n",
       "        antecedent_len  \n",
       "169500               3  \n",
       "225623               2  \n",
       "225621               3  \n",
       "225620               3  \n",
       "225619               3  \n",
       "225618               3  \n",
       "225617               3  \n",
       "225616               3  \n",
       "225615               3  \n",
       "225614               3  \n",
       "225613               3  \n",
       "225612               3  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh2 = RecommenderSystem(2)\n",
    "hh2.metric = 'confidence'\n",
    "hh2.assoc_table.sort_values(hh2.metric, ascending=False).head(12)\n",
    "#This table changes slightly without max_len=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe07b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
